---
layout: post
title: Markov Decision Process(MDP)
comments: true
categories: [DataScience/Reinforcement Learning]
tags: [MDP, Reinforcement Learning, 강화학습, rl]
typora-root-url: ../../assets/img/스크린샷 2021-10-15 오후 4.36.44.png
---

<br/>

강화학습에서 agent가 학습을 하는데에 있어서 가장 중요한것은 agent가 풀고자 하는 문제의 정의라고 할 수 있다.

문제가 정의가 되어야지 학습을 시작할 수 있기 때문이다

사람은 스스로 문제에 대해서 정의를 내릴 수 있지만, 컴퓨터인 agent는 그러기 쉽지 않다.

따라서 우리가 직접 문제의 정의를 많지도 않고 적지도 않게 학습을 할만큼 정도로 직접 해주어야한다

<br/>

# MDP가 무엇인가?

앞선 글에서 말했듯이 강화학습은 순차적 행동 결정 문제라고 하였다.

여기서 말하는 MDP는 순차적으로 이루어지는 행동들을 결정하는 것을 모델링을 한 수학적 프레임 워크라고 할 수 있다.

또한 중요한건 이 행동들의 결정은 stochastic 한 방법으로 결정된다는 것이다.

어렵게 생각하지 말고 그냥 한마디로 강화학습이라는 추상적인 개념을 수식으로 나타내었다고 보면 된다.

덕분에 우리는 이 MDP라는 수학적 프레임 워크를 통해 문제를 정의 할 수 있게된다.

<br/>

<br/>

<br/>

# MDP의 구성요소

![MDP의 구성 요소](https://github.com/aLVINlEE9/aLVINlEE9.github.io/blob/master/assets/img/DS-Reinforcement%20Learning/2021-10-15-rlpost2-01.png?raw=true "MDP의 구성 요소")

MDP의 구성요소도 마찬가지로 앞선 글에서의 강화학습의 구성요소와 거의 유사하다.

하지만 MDP에선 보상함수, 상태변환확률, 할인율이라는 개념이 더 추가가 되었다. (좀 더 확장 되었다 생각해도 된다.)

따라서 하나 하나를 grid-world를 예를 들어 다시 설명을 해 볼것이다.

<br/>

## 상태 (state)

<img src="https://github.com/aLVINlEE9/aLVINlEE9.github.io/blob/master/assets/img/DS-Reinforcement%20Learning/2021-10-15-rlpost2-03.png?raw=true" alt="2021-10-15-rlpost2-03.png" style="zoom: 33%;" />

이와 같은 grid-world가 있다고 가정하자.

agent가 갈 수 있는 State중에서 **(1, 3)** 지점을 MDP의 수학적 표현을 빌리게 되면,

### <center>S<sub>t</sub> = (1, 3)</center> 

이런식으로 나타낼 수 있다.

다시 해석해보자면 시간 **t**일때 (1, 3)인 지점을 우리는 위 처럼 표현을 한다.

그런데 일반적으로 특정 시간 **t**에서의 상태 **S<sub>t</sub>** 가 어떤 상태에 있는지는 아무도 모른다. ( (1, 1), (5, 5) 등...).

그런 정해져있지 않은  **S<sub>t</sub>** 를 가지고 우리는 확률변수(random variable) 이라고 한다.

그러면  실제 (1,3) 과 같이 정해진 상태를 어떻게 표현을 할까? 

아래와 같이 소문자 **s**로 표현이 가능하다.

### <center> S<sub>t</sub> = s</center> 

그리고 아래를 보면 특정 시간에서의 정해진 특정 상태를 소문자와 첨자를 통해 표현도 가능하다.

### <center>s<sub>1</sub> = (1,3)<sub>t = 1</sub> </center>   

방금까지 말한 내용은 아직까진 MDP에 대한 직접적인 내용은 없지만 앞으로 설명할 때에 필요한 개념이므로 생각을 해놔야한다.

<br/>

## 행동(action)

위의 grid-world에서의 action set은 4가지 밖에 없다.

### <center>*A* = {up, down, right, left} </center>

따라서 행동 집합을 위와 같이 나타낼 수 있다.

그럼 우리가 배운 내용을 바탕으로 어떤 state에서 action을 한 후 agent가 이동하는 과정을 생각해보자.

<img src="https://github.com/aLVINlEE9/aLVINlEE9.github.io/blob/master/assets/img/DS-Reinforcement%20Learning/2021-10-15-rlpost2-07.png?raw=true" alt="2021-10-15-rlpost2-07.png" style="zoom:50%;" />

이와 같이 agent가 **(3,1)state** 에서 **(4,1)state**로 이동하는 과정에서 right라는 acion을 하였다.

이런 추상적인 말을 수학적으로 표현해본다면 어떻게 될까?

(3,1)또는 (4,1)이라는 state 또한 정해진 state이기 때문에 우리는 소문자로 나타낼 수 있다고 말했다.

따라서 현재 상태를 **s<sub>1</sub> = (3,1)** 이라고 하고 행동 후 상태를 **s<sub>2</sub> = (4,1)**이라고 표현을 하면,

시간 t일때 agent는  **S<sub>t</sub> = s<sub>1</sub>** 에서 **A<sub>t</sub> = right** 라는 행동을 해서 시간 t + 1일때  **S<sub>t+1</sub> = s<sub>2</sub>** 으로 이동 하였다고 할 수 있다.

여기서 **A<sub>t</sub>** 도 random variable인 셈이다.

여기 까지 이해를 해야지 다음 상태 변환 확률을 이해할 수 있다.

<br/>

## 상태 변환 확률 (state transition probablity)

방금 action을 배웠다.

우리는 agent가 어떤 상태에서 어떤 행동을 취한다면 agent의 상태가 변한다는것을 알고있다.

<img src="https://github.com/aLVINlEE9/aLVINlEE9.github.io/blob/master/assets/img/DS-Reinforcement%20Learning/2021-10-15-rlpost2-07.png?raw=true" alt="2021-10-15-rlpost2-07.png" style="zoom:50%;" />

이 grid world 예제를 계속해서 이용을 하겠다.

아까 예제 처럼 시간 t일때 agent는  **S<sub>t</sub> = s<sub>1</sub>** 에서 **A<sub>t</sub> = right** 라는 행동을 해서 시간 t + 1일때  **S<sub>t+1</sub> = s<sub>2</sub>** 으로 이동 하였다고 할 수 있다 하였다.

근데 agent가 우리가 명령한대로 행동을 안할 확률도 포함 되어있다고 가정을 하게 된다면 말이 달라지게 된다.

가령 right라고 action을 넣었지만  agent가 갑자기 넘어지는 바람에 left를 할 수 도 있다는 말인것이다.

이처럼 상태의 변화에는 확률적인 요인이 들어가게 되는데, 이것을 수학적으로 표현한것을 가지고 우리는  상태 변환 확률(state transition probablity) 이라고 하는 것이다.

### <center>상태 변환 확률 = P[S<sub>t+1</sub> = s<sub>2</sub> | S<sub>t</sub> = s<sub>1</sub> , A<sub>t</sub> = a] </center>

이 값은 agent 가 알지 못하는 값이므로 환경의 일부라고 생각해야한다.

<br/>

## 보상함수 (reward function)



------

*제가 올린 글에서 잘못된 부분이 있으면 제 메일로 연락주세요!*

*Reference : 파이썬과 케라스로 배우는 강화학습*