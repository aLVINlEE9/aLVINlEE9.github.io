---
layout: post
title: Markov Decision Process(MDP)
comments: true
categories: [DataScience/Reinforcement Learning]
tags: [MDP, Reinforcement Learning, 강화학습, rl]
typora-root-url: ../../assets/img/스크린샷 2021-10-15 오후 4.36.44.png
---

<br/>

강화학습에서 agent가 학습을 하는데에 있어서 가장 중요한것은 agent가 풀고자 하는 문제의 정의라고 할 수 있다.

문제가 정의가 되어야지 학습을 시작할 수 있기 때문이다

사람은 스스로 문제에 대해서 정의를 내릴 수 있지만, 컴퓨터인 agent는 그러기 쉽지 않다.

따라서 우리가 직접 문제의 정의를 많지도 않고 적지도 않게 학습을 할만큼 정도로 직접 해주어야한다

<br/>

# MDP가 무엇인가?

앞선 글에서 말했듯이 강화학습은 순차적 행동 결정 문제라고 하였다.

여기서 말하는 MDP는 순차적으로 이루어지는 행동들을 결정하는 것을 모델링을 한 수학적 프레임 워크라고 할 수 있다.

또한 중요한건 이 행동들의 결정은 stochastic 한 방법으로 결정된다는 것이다.

어렵게 생각하지 말고 그냥 한마디로 강화학습이라는 추상적인 개념을 수식으로 나타내었다고 보면 된다.

덕분에 우리는 이 MDP라는 수학적 프레임 워크를 통해 문제를 정의 할 수 있게된다.

![image](/../스크린샷 2021-10-15 오후 4.36.44.png)

MDP의 구성요소도 마찬가지로 앞선 글에서의 강화학습의 구성요소와 거의 유사하다.

하지만 MDP에선 보상함수, 상태변환확률, 할인율이라는 개념이 더 추가가 되었다. (좀 더 확장 되었다 생각해도 된다.)

따라서 하나 하나 다시 설명을 해 볼것이다.

## 