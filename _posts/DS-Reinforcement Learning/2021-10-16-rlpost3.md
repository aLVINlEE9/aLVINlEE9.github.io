---
layout: post
title: Value Function(가치함수)
comments: true
categories: [DataScience/Reinforcement Learning]
tags: [value function, 가치함수, Reinforcement Learning, 강화학습, rl]
---

<br/>

우리는 앞서 MDP를 통해 강화학습 문제를 정의 하였다.

설정된 MDP를 가지고 우리는 agent가 미래 reward를 고려하여 action을 선택할것이라고 하였다.

그런데 도대체 어떻게 미래 reward를 고려한다는것이고 어떤 방식으로 action을 선택할 것인가?

이번 value function(가치 함수)를 배우게 되면 그 방법을 조금 알게 될것이다.

<br/>

# Value function(가치 함수)가 무엇인가?

### <center>V(s) = E[R<sub>t+1</sub> + γR<sub>t+2</sub> + γ<sup>2</sup> R<sub>t+3</sub> + ... | S<sub>t</sub> = s]</center>

위 식을 해석해보자면 value function은 현재 timestep 이후의 모든 future reward의 기댓값이라고 할 수 있다.

실제 grid-world에서는 한정된 에피소드에서 value값을 얻어 낼것이다.

<img src="https://github.com/aLVINlEE9/aLVINlEE9.github.io/blob/master/assets/img/DS-Reinforcement%20Learning/2021-10-15-rlpost2-09.jpeg?raw=true" alt="2021-10-15-rlpost2-09.jpeg" style="zoom:50%;" />

위 에피소드에서 S<sub>t=0</sub> = (1, 1)일때의 value값을 구해보자.(γ = 0.8)

V(s<sub>(1, 1)</sub>) = E[0 + 0.8 * 0 + (0.8)<sup>2</sup> * 1] = 0.64

여기서 R<sub>t+1</sub> 인 R<sub>1</sub>는 state (1, 2)의 reward 값인데 없으므로 0이다.

R<sub>t+2</sub> 인 R<sub>2</sub> 는 state (1, 3)의 reward 값인데 없으므로 0이다. 그리고 discounting factor(할인율) 이 0.8이므로 곱하면 0이 된다.

R<sub>t+3</sub> 인 R<sub>3</sub> 는 state (2, 3)의 reward 값인데 1이다. 그리고 discounting factor(할인율)이 0.8<sup>2</sup> 이므로 곱하면 0.64가 된다.

예제를 보았다.

우리는 저 가치함수를 좀 더 간단히 점화식 형태로 만들 수 있을것 같다.

### <center>V(s) = E[R<sub>t+1</sub> + γV(S<sub>t+1</sub>) | S<sub>t</sub> = s]</center>

이렇게 γ를 묶어서 다음 state의 value function으로 넣어주게 되면 위 식처럼 간단히 표현 할 수 있다.

여기 까지는 정책을 고려하지 않은 가치함수였다.

그러나 agent가 행동을 선택하기 위해서는 정책을 꼭 고려해야한다. (에피소드가 진행 중인 경우)

<br/>

<br/>

<br/>

# 정책을 고려한 가치함수

다음 예시를 보면 왜 정책을 고려해야하는지 알게 될 것이다.

<img src="https://github.com/aLVINlEE9/aLVINlEE9.github.io/blob/master/assets/img/DS-Reinforcement%20Learning/2021-10-16-rlpost3-01.jpg?raw=true" alt="2021-10-16-rlpost3-01.jpg" style="zoom: 33%;" />

V(s) = E[R<sub>t+1</sub> + γV(S<sub>t+1</sub>) | S<sub>t</sub> = (2, 2)]

이 식을 위 예시로 적용해보면 문제가 하나 생긴다.

S<sub>t</sub> 같은 경우는 (2, 2)로 정해져 있는 state 이지만,

S<sub>t+1</sub> 는 random variable 이기 때문에 action(up, down, right, left)이 무엇이냐에 따라 4가지의 선택지가 있다.

그것을 우리는 policy와 state transition probability가 정해준다는 것을 이미 알고 있다.

어떻게일까??

<br/>

## Policy를 고려

먼저 policy를 보자

### <center> π(a | s) = P[A<sub>t</sub> = a | S<sub>t</sub> = s]</center>

이것은 policy의 식이다.

해석해보자면 특정 state일때 특정 action을 할 확률이다.

이 예시에 적용해보면 (2, 2)에서 4가지 방향으로 갈 확률을 정책이라고 할 수 있다.

이 policy를 고려하면 이제 우리는 random variable을 확률이 높은 정책으로 선택을 해서 S<sub>t+1</sub> 을 해결 할 수 있다.(*policy iteration*)

지금까지 policy를 고려했지만, 한번더 생각해보면 S<sub>t+1</sub>  을 정하는 요소가 하나더 있다.

<br/>

## State Transition probability를 고려

아까 말했듯이 state transition probability(상태 변환 확률) 이다.

### <center>상태 변환 확률 = P[S<sub>t+1</sub> = s<sub>2</sub> | S<sub>t</sub> = s<sub>1</sub> , A<sub>t</sub> = a]</center>

해석해보자면, 특정 state = s<sub>1</sub> 에서 특정 action = a 을 했을때 특정 state = s<sub>2</sub> 이될 확률이다.

상태 변환 확률까지 고려해야 완벽해진다.

<br/>

그래서 이 policy를 고려한 value function은 다음과 같다.

### <center>V<sub>π</sub>(s) = E[R<sub>t+1</sub> + γV<sub>π</sub>(S<sub>t+1</sub>) | S<sub>t</sub> = s]</center>

이것이 바로 나중에 다룰 벨만 기대 방정식(Bellman Expectation Equation)이다.

<br/>

<br/>

<br/>

# Value function의 활용

value function을 통해 우리는 각 state의 가치를 알게 되었고,

agent는 각 state 의 가치들를 통해 action을 선택할 수 있는 정보를 제공 받게 된다.

따라서 어떤 state가 가장 최적인지를 알려주게 된다.

지금까지 설명한 가치함수는 상태 가치함수 (state value-function)이다

각 state에 대해서만 가치를 구한것이다.

그런데 각 state에서 했던 action에 대해 가치를 매기면 좀더 agent가 정확한 선택을 할것 같지 않은가?

그걸 이제 우리는 Q-function이라고 하고 이 부분에 대해서는 다음 포스트에 업로드할 예정이다.

<br/>

------

*제가 올린 글에서 잘못된 부분이 있으면 제 메일로 연락주세요!*

*Reference : 파이썬과 케라스로 배우는 강화학습*
