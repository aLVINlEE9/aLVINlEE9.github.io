---
layout: post
title: Value Function(가치함수)
comments: true
categories: [DataScience/Reinforcement Learning]
tags: [value function, 가치함수, Reinforcement Learning, 강화학습, rl]
---

<br/>

우리는 앞서 MDP를 통해 강화학습 문제를 정의 하였다.

설정된 MDP를 가지고 우리는 agent가 미래 reward를 고려하여 action을 선택할것이라고 하였다.

그런데 도대체 어떻게 미래 reward를 고려한다는것이고 어떤 방식으로 action을 선택할 것인가?

이번 value function(가치 함수)를 배우게 되면 그 방법을 조금 알게 될것이다.

# Value function(가치 함수)가 무엇인가?

### <center>V(s) = E[R<sub>t+1</sub> + γR<sub>t+2</sub> + γ<sup>2</sup> R<sub>t+3</sub> + ... | S<sub>t</sub> = s]</center>

위 식을 해석해보자면 value function은 현재 timestep 이후의 모든 future reward의 기댓값이라고 할 수 있다.

실제 grid-world에서는 한정된 에피소드에서 value값을 얻어 낼것이다.

<img src="https://github.com/aLVINlEE9/aLVINlEE9.github.io/blob/master/assets/img/DS-Reinforcement%20Learning/2021-10-15-rlpost2-09.jpeg?raw=true" alt="2021-10-15-rlpost2-09.jpeg" style="zoom:50%;" />

위 에피소드에서 S<sub>t=0</sub> = (1, 1)일때의 value값을 구해보자.(γ = 0.8)

V(s<sub>(1, 1)</sub>) = E[0 + 0.8 * 0 + (0.8)<sup>2</sup> * 1] = 0.64

여기서 R<sub>t+1</sub> 인 R<sub>1</sub>는 state (1, 2)의 reward 값인데 없으므로 0이다.

R<sub>t+2</sub> 인 R<sub>2</sub> 는 state (1, 3)의 reward 값인데 없으므로 0이다. 그리고 discounting factor(할인율) 이 0.8이므로 곱하면 0이 된다.

R<sub>t+3</sub> 인 R<sub>3</sub> 는 state (2, 3)의 reward 값인데 1이다. 그리고 discounting factor(할인율)이 0.8<sup>2</sup> 이므로 곱하면 0.64가 된다.

예제를 보았다.

우리는 저 가치함수를 좀 더 간단히 점화식 형태로 만들 수 있을것 같다.

### <center>V(s) = E[R<sub>t+1</sub> + γV(S<sub>t+1</sub>) | S<sub>t</sub> = s]</center>

이렇게 γ를 묶어서 다음 state의 value function으로 넣어주게 되면 위 식처럼 간단히 표현 할 수 있다.

여기 까지는 정책을 고려하지 않은 가치함수였습니다.

그러나 agent가 행동을 선택하기 위해서는 정책을 꼭 고려해야합니다.

