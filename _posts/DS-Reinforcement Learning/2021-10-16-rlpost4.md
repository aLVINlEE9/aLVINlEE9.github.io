---
layout: post
title: Q-function(큐함수)
comments: true
categories: [DataScience/Reinforcement Learning]
tags: [q function, 큐함수, Reinforcement Learning, 강화학습, rl]
date: 2021-10-16 21:00:00
---

<br/>

우리는 앞서 상태 가치 함수(state value-function)에 대해서 공부하였다.

agent는 가치함수를 통해 어떤 state에 있는게 좋은지를 판단 할 수 있다.

그러나 우리는 그 state에 있을때 어떤 action을 하는게 좋은지를 판단 할 수 있더라면 agent가 더 정확하게 action을 선택하지 않을까라는 생각을 했었다.

그래서 나온 개념이 Q-function이다.

<br/>

# Q-function이 무엇인가?

![2021-10-16-rlpost4-01.png](https://github.com/aLVINlEE9/aLVINlEE9.github.io/blob/master/assets/img/DS-Reinforcement%20Learning/2021-10-16-rlpost4-01.png?raw=true)

### <center>V<sub>π</sub>(s) = E[R<sub>t+1</sub> + γV<sub>π</sub>(S<sub>t+1</sub>) | S<sub>t</sub> = s]</center>

### <center>q<sub>π</sub>(s, a) = E<sub>π</sub>[R<sub>t+1</sub> + γq<sub>π</sub>(S<sub>t+1</sub>, A<sub>t+1</sub>) | S<sub>t</sub> = s, A<sub>t</sub> = a]</center>

첫번쨰 식은 state value function이고

두번째 식은 state action function(Q function)이다.

두 식을 비교해보면 그냥 action이 하나 추가된것이 다이다.

그럼 속에는 무슨 의미를 포함하고 있을까?

q function는 행동에 대한 가치를 나타낸 것이라 할 수 있다.

여기서 중요한것은 |(given)의 오른쪽부분에 있는 조건에서 action이 추가되었다는 것이다.

그렇기 때문에 q function은 상태에 대한 가치가 아닌 상태에서 특정한 행동을 했을때의 가치를 구하게 되는 것이다.

<br/>

## Value-function vs Q-function

value function과 q function의 관계는 다음과 같다.

### <center>V<sub>π</sub>(s) = Σ<sub>a∈A</sub> π(a | s) q<sub>π</sub> (s, a)</center>

그냥 현재 state 에서 모든 행동들에 대한 q값들의 합이 value라고 생각하면 된다

<br/>

<br/>

<br/>

# 그래서 Q-function 쓰는 이유?

value-function 같은 경우 고려하는게 state 하나 뿐이다. 다른말로 action과의 연결성이 없다고도 할 수 있다.

따라서 여러가지 요인에 의해 값이 왜곡될 수 있다.

예를들면 policy에 의해서 action이 바뀔 수도 있고, state transition probability에 의해서도 action이 달라질 수 있다.

또한 각 state에 대한 value값을 알아야 하기도 한다.

그러나 q-function을 이용하게 되면 실제 각 action에 대해서 가치를 알려주어서 action 좀더 정확히 선택 할 수 있게 된다.

다음 포스트에는 Bellman Equation 에 대해서 설명할 것이다.

<br/>

------

*제가 올린 글에서 잘못된 부분이 있으면 제 메일로 연락주세요!*

*Reference : 파이썬과 케라스로 배우는 강화학습*
