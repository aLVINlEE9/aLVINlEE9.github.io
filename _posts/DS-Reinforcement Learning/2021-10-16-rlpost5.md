---
layout: post
title: Bellman Equation(벨만 방정식)
comments: true
categories: [DataScience/Reinforcement Learning]
tags: [bellman equation, bellman expectation equation, bellman optimality equation, 벨만 방정식, 벨만 기대방정식, 벨만 최적방정식, Reinforcement Learning, 강화학습, rl]
---

<br/>

지금까지 설명한 내용들을 이제 계산 가능한 식의 형태로 나타내 볼테다.

<br/>

# 벨만 기대 방정식(Bellman Expectation Equation)

우리는 "value-function을 벨만 기대 방정식(Bellman Expectation Equation)이다." 라고 value-function을 설명하던 중 언급 한 적이 있다.

말 그대로

### <center>V<sub>π</sub>(s) = E[R<sub>t+1</sub> + γV<sub>π</sub>(S<sub>t+1</sub>) | S<sub>t</sub> = s]</center>

이것이 벨만 기대 방정식(Bellman Expectation Equation)이라고 한다.

**E** (기댓값)을 계산 가능하게 바꾸게 된다면 아래와 같은 수식이 성립하게 된다.

### <center>V<sub>π</sub>(s) = Σ<sub>a∈A</sub> π(A<sub>t</sub> | S<sub>t</sub> = s) (r(S<sub>t</sub> = s, A<sub>t</sub> = a) + γΣ<sub>s'∈S<sub>t+1</sub></sub> P(S<sub>t+1</sub> = s' | S<sub>t</sub> = s, A<sub>t</sub> = a)V<sub>π</sub>(s'))</center>

위 수식을 해석하자면,

현재 state에서 선택된 각각의 action을 하였을때의 reward에다가,

discounting factor에 (현재 state에서 action을 하였을때 다음 state가 s'일 확률인) 상태변환 확률을 곱하고 그 다음 state를 고려한 V<sub>π</sub> 또한 곱해서 앞의 reward와 합해준다.

그다음 policy를 통해 얻을 수있는 모든 action들에 대한 확률을 전체에 곱해주면 value 값을 실제로 계산 할 수 있다.

쉽게 말하면, state transition probabilty 와 모든 action에 대한 policy 확률을 고려해서 구했다고 하면 된다.

## Grid-world 예제



------

*제가 올린 글에서 잘못된 부분이 있으면 제 메일로 연락주세요!*

*Reference : 파이썬과 케라스로 배우는 강화학습*

