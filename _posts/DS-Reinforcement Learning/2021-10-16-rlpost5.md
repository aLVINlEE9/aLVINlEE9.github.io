---
layout: post
title: Bellman Equation(벨만 방정식)
comments: true
categories: [DataScience/Reinforcement Learning]
tags: [bellman equation, bellman expectation equation, bellman optimality equation, 벨만 방정식, 벨만 기대방정식, 벨만 최적방정식, Reinforcement Learning, 강화학습, rl]
---

<br/>

지금까지 설명한 내용들을 이제 계산 가능한 식의 형태로 나타내 볼테다.

<br/>

# 벨만 기대 방정식(Bellman Expectation Equation)

우리는 "value-function을 벨만 기대 방정식(Bellman Expectation Equation)이다." 라고 value-function을 설명하던 중 언급 한 적이 있다.

말 그대로

### <center>V<sub>π</sub>(s) = E[R<sub>t+1</sub> + γV<sub>π</sub>(S<sub>t+1</sub>) | S<sub>t</sub> = s]</center>

이것이 벨만 기대 방정식(Bellman Expectation Equation)이라고 한다.

**E** (기댓값)을 계산 가능하게 바꾸게 된다면 아래와 같은 수식이 성립하게 된다.

### <center>V<sub>π</sub>(s) = Σ<sub>a∈A</sub> π(A<sub>t</sub> | S<sub>t</sub> = s) (r(S<sub>t</sub> = s, A<sub>t</sub> = a) + γΣ<sub>s'∈S<sub>t+1</sub></sub> P(S<sub>t+1</sub> = s' | S<sub>t</sub> = s, A<sub>t</sub> = a)V<sub>π</sub>(s'))</center>

위 수식을 해석하자면,

각 행동에 대해 그 행동을 할 확률을 고려하고 행동을 했을떄 받을 보상과 상태 변환확률을 고려한 다음 상태의 가치함수를 고려한다.

쉽게 말하면, state transition probabilty 와 모든 action에 대한 policy 확률을 고려해서 구했다고 하면 된다.

<br/>

## Grid-world 예제

![2021-10-16-rlpost5-01.png](https://github.com/aLVINlEE9/aLVINlEE9.github.io/blob/master/assets/img/DS-Reinforcement%20Learning/2021-10-16-rlpost5-01.png?raw=true)

여기서는 statet tarnsition probability 를 1로 설정하고 할 것이다.

그리고 최초 policy는 0.25로 두고 γ는 0.9이다.

그럼 다음과 같은 식이 나올 것이다.

### <center>V<sub>π</sub>(s) = Σ<sub>a∈A</sub> π(A<sub>t</sub> | S<sub>t</sub> = s) (r(S<sub>t</sub> = s, A<sub>t</sub> = a) + γV<sub>π</sub>(s'))</center>

| Action    | Value                                  |
| --------- | -------------------------------------- |
| up(Q)     | 0.25 * (0 + 0.9 * 0) = **0**           |
| down(Q)   | 0.25 * (0 + 0.9 * 0.5) = **0.1125**    |
| left(Q)   | 0.25 * (0 + 0.9 * 1) = **0.225**       |
| right(Q)  | 0.25 * (1 + 0.9 * 0) = **0.25**        |
| **Value** | 0 + 0.1125 + 0.225 + 0.25 = **0.5875** |



------

*제가 올린 글에서 잘못된 부분이 있으면 제 메일로 연락주세요!*

*Reference : 파이썬과 케라스로 배우는 강화학습*

