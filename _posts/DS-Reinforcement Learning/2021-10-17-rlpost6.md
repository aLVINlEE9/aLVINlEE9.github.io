---
layout: post
title: Policy Iteration(정책 이터레이션)<수정중>
comments: true
categories: [DataScience/Reinforcement Learning]
tags: [policy iteration, 정책 이터레이션, Reinforcement Learning, 강화학습, rl]
---

<br/>

전 포스트에서 벨만 기대 방정식에 대해서 다루어 보았다.

### <center>V<sub>π</sub>(s) = E[R<sub>t+1</sub> + γV<sub>π</sub>(S<sub>t+1</sub>) | S<sub>t</sub> = s]</center>

이는 벨만 기대 방정식이다.

우리는 벨만 기대 방정식을 통해 한 timestep의 각 state의 value값을 구할 수 있다.

그러나 그걸가지고 어떻게 강화학습의 목적인 최적인 action을 선택 하게 학습을 시킬것인가?

우리는 앞에서 정책이라는 것을 배웠다.

정책 무엇이였냐면 "모든 상태에 대해 agent가 어떤 행동을 해야 하는지 정해놓은 규칙" 이였다.

그럼 우리는 정책을 얼마나 잘 정하는지에 따라 최적인 action을 선택할 것임을 깨달을 수 있다.

따라서 이번엔 벨만 기대 방정식을 통해 정책과 value값을 계산하는 과정을 배울 것이다.

<br/>

# Policy Iteration

![2021-10-17-rlpost6-01.png](https://github.com/aLVINlEE9/aLVINlEE9.github.io/blob/master/assets/img/DS-Reinforcement%20Learning/2021-10-17-rlpost6-01.png?raw=true)

우리는 벨반 방정식으로 value 값을 얻어낼 수 있다.

그런데 그 value 값만 가지고는 agent가 행동하는ㄷ

현재 정책에 따라 행동 했을때의 value값을 얻는 정책 평가와 정책평가를 해서 얻은 value값을 바탕으로 다시 정책을 업데이트 시키는 정책 발전을 계속해서 반복을 하는 policy iteration(정책 반복)을 할 것이다.

이를 통해 최적의 정책을 찾아 agent가 최대한 효율적으로 움직이게 끔 학습을 시키는것이 목적이다.





------

*제가 올린 글에서 잘못된 부분이 있으면 제 메일로 연락주세요!*

*Reference : 파이썬과 케라스로 배우는 강화학습*

