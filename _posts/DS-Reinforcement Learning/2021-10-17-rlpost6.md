---
layout: post
title: Policy Iteration(정책 이터레이션)
comments: true
categories: [DataScience/Reinforcement Learning]
tags: [policy iteration, 정책 이터레이션, Reinforcement Learning, 강화학습, rl]
---

<br/>

전 포스트에서 벨만 기대 방정식에 대해서 다루어 보았다.

### <center>V<sub>π</sub>(s) = E[R<sub>t+1</sub> + γV<sub>π</sub>(S<sub>t+1</sub>) | S<sub>t</sub> = s]</center>

이는 벨만 기대 방정식입니다. state-action value function이라고도 한다.

왜 state-action인것일까?

그 이유는 식에 포함되어있는 π 때문 인데, π는 정책이 포함되어있다라는 의미이다.

그럼 정책에 대해서 생각해보자.

정책 무엇이였냐면 "모든 상태에 대해 agent가 어떤 행동을 해야 하는지 정해놓은 규칙" 이였다.

다시 한번 강화학습의 목적을 생각해보자.

우리가 왜 강화학습을 하는것인가?

agent가 action을 최대한 효율적으로 움직이도록 하기위해 강화학습을 한다.

이번에는 value값을 통해 정책을 업데이트 시키고, 업데이트 된 정책을 통해 다시 value값을 얻는 policy iteration(정책 반복)을 하게 될것이다.

이를 통해 최적의 정책을 찾아 agent가 최대한 효율적으로 움직이게 끔 학습을 시키는것이 목적이다.

------

*제가 올린 글에서 잘못된 부분이 있으면 제 메일로 연락주세요!*

*Reference : 파이썬과 케라스로 배우는 강화학습*

