---
layout: post
title: Monte Carlo Prediction and Temporal Difference Error
comments: true
categories: [DataScience/Reinforcement Learning]
tags: [Monte Carlo, Temporal Difference Error, mc, td error, Reinforcement Learning, 강화학습, rl]

---

<br/>

우리는 policy iteration을 policy evaluation과 policy improvemet를 통해서 다이나믹 프로그래밍을 이용해 계산을 하였다.

그런데, 대부분의 문제는 다이나믹 프로그래밍을 적용하기 어렵다.

그 이유는 state가 너무 많을 수 도 차원이 증가할 수록 계산복잡도가 기하 급수적으로 증가하기 때문이다.

그럼 다이나믹 프로그래밍 같은 방법 말고 다른 방법이 있을까?

실제 사람은 cpu와 같이 계산을 빠르게 할 줄도 모르는데 어떻게 학습을 하는지를 생각 해보면 된다.

바둑을 아예 모르는 사람을 예를 들면,

그 사람은 일단 바둑을 해볼 것이다.

그리고 자신을 평가하고,

평가한대로 자신을 업데이트 하면서 이과정을 계속 반복해 나갈 것이다.

일단 해본다고 했었는데 그것이 이번에 설명할 몬테카를로에 관련된 부분이다.





<br/>

------

*제가 올린 글에서 잘못된 부분이 있으면 제 메일로 연락주세요!*

*Reference : 파이썬과 케라스로 배우는 강화학습*