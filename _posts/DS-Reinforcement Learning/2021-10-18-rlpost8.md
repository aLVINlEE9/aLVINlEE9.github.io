---
layout: post
title: Monte Carlo Prediction and Temporal Difference Error
comments: true
categories: [DataScience/Reinforcement Learning]
tags: [Monte Carlo, Temporal Difference Error, mc, td error, Reinforcement Learning, 강화학습, rl]

---

<br/>

우리는 policy iteration을 policy evaluation과 policy improvemet를 통해서 다이나믹 프로그래밍을 이용해 계산을 하였다.

그런데, 대부분의 문제는 다이나믹 프로그래밍을 적용하기 어렵다.

그 이유는 state가 너무 많을 수 도 차원이 증가할 수록 계산복잡도가 기하 급수적으로 증가하기 때문이다.

그럼 다이나믹 프로그래밍 같은 방법 말고 다른 방법이 있을까?

실제 사람은 cpu와 같이 계산을 빠르게 할 줄도 모르는데 어떻게 학습을 하는지를 생각 해보면 된다.

바둑을 아예 모르는 사람을 예를 들면,

그 사람은 일단 바둑을 해볼 것이다.

그리고 자신을 평가하고,

평가한대로 자신을 업데이트 하면서 이과정을 계속 반복해 나갈 것이다.

일단 해본다고 했었는데 그것이 이번에 설명할 몬테카를로에 관련된 부분이다.

<br/>

# 몬테카를로 근사

![2021-10-18-rlpost8-01.png](https://github.com/aLVINlEE9/aLVINlEE9.github.io/blob/master/assets/img/DS-Reinforcement%20Learning/2021-10-18-rlpost8-01.png?raw=true)

우리가 이렇게 생긴 A의 넓이를 어떻게 구할 수 있을까?

적분을 하면 매우 복잡해 질것이다.

따라서 몬테카를로 근사를 통해 넓이를 구할 수 있는데,

저 직사각형 선안에 무작위로 점을 찍는다. 이것을 나중에 sampling이라고 한다.

충분히 많은 sampling을 얻으면 우리는 A에 찍힌 점들의 갯수와 직사각형의 넓이만 알면 쉽게 A의 넓이의 근사치를 구할 수 있다.

### <center>A의 넓이 = 직사각형 넓이 * (A찍힌 점의 갯수 / 전체 찍은 점의 갯수)</center>

<br/>

<br/>

<br/>



# 몬테카를로 예측

우리는 아까 몬테카를로 근사를 이용해서 한반도의 넓이의 근사치를 구하는 식을 만들어 보았다. 

그럼 이 몬테카를로 근사를 policy evaluation에서 가치함수의 근사치를 구할 수 도 있을것 같다.

우리가 한반도 넓이를 구할 때 점하나를 찍는것을 가지고 sampling이라고 하였다.

그럼 가치함수의 근사치를 구할때는 어떻게 sampling을 해야할까?

agent가 한번의 에피소드를 간것을 가지고 sampling의 기준을 세워보면 될것같다.

여러번의 에피소드를 하면 충분한 sampling을 얻을 것이다. 

이 얻은 sample들의 평균으로 참 가치함수를 추정할 수 있는데, 이것을 가지고 우리는 몬테카를로 예측이라고 한다.

<br/>

------

*제가 올린 글에서 잘못된 부분이 있으면 제 메일로 연락주세요!*

*Reference : 파이썬과 케라스로 배우는 강화학습*