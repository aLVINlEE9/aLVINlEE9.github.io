---
layout: post
title: SARSA(살사)
comments: true
categories: [DataScience/Reinforcement Learning]
tags: [salsa, 살사, Reinforcement Learning, 강화학습, rl]
date: 2021-10-18 16:00:00


---

<br/>

이 밑의 그림은 살사와 큐러닝의 흐름도이다.

![2021-10-18-rlpost9-01.png](https://github.com/aLVINlEE9/aLVINlEE9.github.io/blob/master/assets/img/DS-Reinforcement%20Learning/2021-10-18-rlpost9-01.png?raw=true)

앞서 mc method와 td-learning에 대해서 배웠다.

td-learning 을 사용하는 두가지 방식인 살사와 큐러닝에 대해서 알게 될텐데,

이번에는 살사에 대해서 배워보겠다.

그전에 epsilon greedy에 대해서 배우겠다.



<br/>



# Epsilon Greedy Policy

입실론 탐욕정책은 원래의 탐욕정책과 약간은 다르다.

![2021-10-18-rlpost9-03.png](https://github.com/aLVINlEE9/aLVINlEE9.github.io/blob/master/assets/img/DS-Reinforcement%20Learning/2021-10-18-rlpost9-03.png?raw=true)

이 식이 모든것을 설명해 준다.

원래 탐욕정책이란 q값이 가장 높은 행동을 하는것인데,

입실론 탐욕정책은 비슷하면서도 입실론(아주 작은)의 확률로 무작위의 행동을 하는 것이 추가가 되었다.

그 이유는 무엇일까?

![2021-10-18-rlpost9-02.png](https://github.com/aLVINlEE9/aLVINlEE9.github.io/blob/master/assets/img/DS-Reinforcement%20Learning/2021-10-18-rlpost9-02.png?raw=true)

그냥 탐욕정책은 위의 식과 같다.

여기서는 q값이 가장 높은 행동 정해진 1가지만 하게 끔 되어있다.

그렇기 때문에 agent가 좀더 많이 탐험을 하지 못하게 된다.

환경을 모르는 agent한테는 다양한 행동을 하면서 환경을 탐험시키는것이 중요한데,

정해진 행동만 하게 되면 이상하게 학습되기 시작한다면 계속해서 그렇게 학습이 되어버리므로,

입실론이라는 작은 확률을 통해서 무작위의 행동을 하게 하여서 다양한 정보를 얻게 도와줄 것이다.

<br/>

<br/>

<br/>

# SALSA

![2021-10-18-rlpost9-04.png](https://github.com/aLVINlEE9/aLVINlEE9.github.io/blob/master/assets/img/DS-Reinforcement%20Learning/2021-10-18-rlpost9-04.png?raw=true)

이것은 salsa의 식이다.

Td-learning을 통해 q값을 업데이트를 해준다고 생각하면 된다.

이름이 왜 salsa 인지는 밑의 사진을 보면 이해가 될것이다.

![2021-10-18-rlpost9-05.png](https://github.com/aLVINlEE9/aLVINlEE9.github.io/blob/master/assets/img/DS-Reinforcement%20Learning/2021-10-18-rlpost9-05.png?raw=true)

이 과정을 실제 grid-world의 예제로 설명을 할것이다.

<br/>

<br/>

<br/>

# SALSA in grid-world

*α = 0.1*

현재 grid world의 q 값이다.

|       |         0         |           |       |         0         |           |       |               0                |       |
| :---: | :---------------: | :-------: | :---: | :---------------: | :-------: | :---: | :----------------------------: | :---: |
| **0** | **s<sub>1</sub>** |   **0**   | **0** | **s<sub>2</sub>** | **-0.02** | **0** |       **s<sub>3</sub>**        | **0** |
|       |       **0**       |           |       |     **-0.01**     |           |       |             **-2**             |       |
|       |       **0**       |           |       |       **0**       |           |       |             **0**              |       |
| **0** | **s<sub>4</sub>** | **-0.01** | **0** | **s<sub>5</sub>** |  **-1**   | **0** | **s<sub>6</sub> Trap(R = -1)** | **0** |
|       |       **0**       |           |       |       **0**       |           |       |             **1**              |       |
|       |       **0**       |           |       |       **0**       |           |       |             **0**              |       |
| **0** | **s<sub>7</sub>** |   **0**   | **0** | **s<sub>8</sub>** |  **0.1**  | **0** | **s<sub>9</sub>  Goal(R = 1)** | **0** |
|       |         0         |           |       |         0         |           |       |               0                |       |

agent가 지나간 state이다.(에피소드)

|   ↓   |       |       |
| :---: | :---: | :---: |
| **↓** | **→** | **↓** |
| **→** | **↑** | **↓** |

<br/>

<br/>

## SALSA 계산

![2021-10-18-rlpost9-04.png](https://github.com/aLVINlEE9/aLVINlEE9.github.io/blob/master/assets/img/DS-Reinforcement%20Learning/2021-10-18-rlpost9-04.png?raw=true)

|       |         0         |              |       |         0         |                  |       |               0                |       |
| :---: | :---------------: | :----------: | :---: | :---------------: | :--------------: | :---: | :----------------------------: | :---: |
| **0** | **s<sub>1</sub>** |    **0**     | **0** | **s<sub>2</sub>** |    **-0.02**     | **0** |       **s<sub>3</sub>**        | **0** |
|       |   **<u>0</u>**    |              |       |     **-0.01**     |                  |       |             **-2**             |       |
|       |       **0**       |              |       |       **0**       |                  |       |             **0**              |       |
| **0** | **s<sub>4</sub>** |  **-0.01**   | **0** | **s<sub>5</sub>** | **<u>-0.92</u>** | **0** | **s<sub>6</sub> Trap(R = -1)** | **0** |
|       |   **<u>0</u>**    |              |       |       **0**       |                  |       |          **<u>1</u>**          |       |
|       |       **0**       |              |       | **<u>-0.08</u>**  |                  |       |             **0**              |       |
| **0** | **s<sub>7</sub>** | **<u>0</u>** | **0** | **s<sub>8</sub>** |     **0.1**      | **0** | **s<sub>9</sub>  Goal(R = 1)** | **0** |
|       |       **0**       |              |       |       **0**       |                  |       |             **0**              |       |

<br/>

#### (Q(s<sub>1</sub>, down) = 0, Q(s<sub>4</sub>, down) = 0, R<sub>t+1</sub> = 0)

### update : Q(s<sub>1</sub>, down) = 0 + 0.1(0 + 0.8*0 - 0) = 0

<br/>

#### (Q(s<sub>4</sub>, down) = 0, Q(s<sub>7</sub>, right) = 0, R<sub>t+1</sub> = 0)

### update : Q(s<sub>4</sub>, down) = 0 + 0.1(0 + 0.8*0 - 0) = 0

<br/>

#### (Q(s<sub>7</sub>, right) = 0, Q(s<sub>8</sub>, up) = 0, R<sub>t+1</sub> = 0)

### update : Q(s<sub>7</sub>, right) = 0 + 0.1(0 + 0.8*0 - 0) = 0

<br/>

#### (Q(s<sub>8</sub>, up) = 0, Q(s<sub>5</sub>, right) = -1, R<sub>t+1</sub> = 0)

### update : Q(s<sub>8</sub>, up) = 0 + 0.1(0 + 0.8*(-1) - 0) = -0.08

<br/>

#### (Q(s<sub>5</sub>, right) = -1, Q(s<sub>6</sub>, down) =1, R<sub>t+1</sub> = -1)

### update : Q(s<sub>5</sub>, right) = -1 + 0.1(-1 + 0.8*(1) - (-1)) = -0.92

<br/>

#### (Q(s<sub>6</sub>, down) = 1, Q(s<sub>9</sub>, A<sub>t+1</sub>) =0, R<sub>t+1</sub> = 1)

### update : Q(s<sub>6</sub>, down) = 1 + 0.1(1 + 0.8*(0) - (1)) = 1

<br/>

------

*제가 올린 글에서 잘못된 부분이 있으면 제 메일로 연락주세요!*

*Reference : 파이썬과 케라스로 배우는 강화학습*