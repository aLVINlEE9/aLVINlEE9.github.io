---
layout: post
title: Q-Learning(큐러닝)
comments: true
categories: [DataScience/Reinforcement Learning]
tags: [q-learning, 큐러닝, Reinforcement Learning, 강화학습, rl]



---

<br/>

이전 포스트에 이어서 이번에는 q-learning을 해보겠다.

<br/>

# SALSA vs Q-learning

q-learning은 SALSA 와 다르게 SALS만 하는것이다,

왜 그러면 마지막 A를 뻈을까??

![2021-10-18-rlpost10-01.png](https://github.com/aLVINlEE9/aLVINlEE9.github.io/blob/master/assets/img/DS-Reinforcement%20Learning/2021-10-18-rlpost10-01.png?raw=true)

왼쪽은 SALSA 오른쪽은 Q-learning이다.

SALSA경우 여기서 큰 문제점이 생기는데,

![2021-10-18-rlpost9-04.png](https://github.com/aLVINlEE9/aLVINlEE9.github.io/blob/master/assets/img/DS-Reinforcement%20Learning/2021-10-18-rlpost9-04.png?raw=true)

이 식을 보다시피 t+1(두번째)에서의 행동이 epsilon greedy policy에 의해서 결정이 되기 때문에 epsilon의 확률로 trap으로 잘못 가게 되면 위 그림과 같이 q값이 직진도 하기전에 마이너스 값으로 설정이되어 agent가 저 길을 포기해 버리게 된다.

이런 큰 문제점을 해결하기 위해서 Q-learning 이라는게 생겼다.

<br/>

<br/>

<br/>

# Q-learning

우리는 방금 salsa의 문제점에 대해서 이야기 해봤다.

그럼 t+1에서의 행동을 효율적으로 선택하여 학습시키는 방법이 어디있을까?

그것이 바로 q-learning이고 우리가 이미 그런 방법을 알고 있다.

바로 벨만 최적 방정식이였다.

두번쨰 행동을 할때 최적 큐를 찾는 식을 이용할 것이다.



![2021-10-18-rlpost10-02.png](https://github.com/aLVINlEE9/aLVINlEE9.github.io/blob/master/assets/img/DS-Reinforcement%20Learning/2021-10-18-rlpost10-02.png?raw=true)

t+1(두번째)의 선택 상황에서 policy 대로 행동을 안하고 최적 큐를 찾는 max를 이용해서 action을 선택하게 되면

![2021-10-18-rlpost10-01.png](https://github.com/aLVINlEE9/aLVINlEE9.github.io/blob/master/assets/img/DS-Reinforcement%20Learning/2021-10-18-rlpost10-01.png?raw=true)

오른쪽과 같이 epsilon의 확률로 trap으로 갈 이유가 전혀 없어지고 학습도 훨씬 정확하게 될 것이다.

그럼 q-learning 또한 grid-world를 이용해서 풀어볼까?

<br/>

<br/>

<br/>

# Q-learning in grid-world

*α = 0.1*

현재 grid world의 q 값이다.

|       |         0         |           |       |         0         |           |       |               0                |       |
| :---: | :---------------: | :-------: | :---: | :---------------: | :-------: | :---: | :----------------------------: | :---: |
| **0** | **s<sub>1</sub>** |   **0**   | **0** | **s<sub>2</sub>** | **-0.02** | **0** |       **s<sub>3</sub>**        | **0** |
|       |       **0**       |           |       |     **-0.01**     |           |       |             **-2**             |       |
|       |       **0**       |           |       |       **0**       |           |       |             **0**              |       |
| **0** | **s<sub>4</sub>** | **-0.01** | **0** | **s<sub>5</sub>** |  **-1**   | **0** | **s<sub>6</sub> Trap(R = -1)** | **0** |
|       |       **0**       |           |       |       **0**       |           |       |             **1**              |       |
|       |       **0**       |           |       |       **0**       |           |       |             **0**              |       |
| **0** | **s<sub>7</sub>** |   **0**   | **0** | **s<sub>8</sub>** |  **0.1**  | **0** | **s<sub>9</sub>  Goal(R = 1)** | **0** |
|       |         0         |           |       |         0         |           |       |               0                |       |

agent가 지나간 state이다.(에피소드)

|   ↓   |       |       |
| :---: | :---: | :---: |
| **↓** | **→** | **↓** |
| **→** | **↑** | **↓** |

<br/>

<br/>

![2021-10-18-rlpost10-02.png](https://github.com/aLVINlEE9/aLVINlEE9.github.io/blob/master/assets/img/DS-Reinforcement%20Learning/2021-10-18-rlpost10-02.png?raw=true)

|       |         0         |                  |       |         0         |                  |       |               0                |       |
| :---: | :---------------: | :--------------: | :---: | :---------------: | :--------------: | :---: | :----------------------------: | :---: |
| **0** | **s<sub>1</sub>** |      **0**       | **0** | **s<sub>2</sub>** |    **-0.02**     | **0** |       **s<sub>3</sub>**        | **0** |
|       |   **<u>0</u>**    |                  |       |     **-0.01**     |                  |       |             **-2**             |       |
|       |       **0**       |                  |       |       **0**       |                  |       |             **0**              |       |
| **0** | **s<sub>4</sub>** |    **-0.01**     | **0** | **s<sub>5</sub>** | **<u>-0.92</u>** | **0** | **s<sub>6</sub> Trap(R = -1)** | **0** |
|       |   **<u>0</u>**    |                  |       |       **0**       |                  |       |          **<u>1</u>**          |       |
|       |       **0**       |                  |       |   **<u>0</u>**    |                  |       |             **0**              |       |
| **0** | **s<sub>7</sub>** | **<u>0.008</u>** | **0** | **s<sub>8</sub>** |     **0.1**      | **0** | **s<sub>9</sub>  Goal(R = 1)** | **0** |
|       |       **0**       |                  |       |       **0**       |                  |       |             **0**              |       |

<br/>

#### (Q(s<sub>1</sub>, down) = 0, maxQ(s<sub>4</sub>, down) = 0, R<sub>t+1</sub> = 0)

### update : Q(s<sub>1</sub>, down) = 0 + 0.1(0 + 0.8*0 - 0) = 0

<br/>

#### (Q(s<sub>4</sub>, down) = 0, maxQ(s<sub>7</sub>, right) = 0, R<sub>t+1</sub> = 0)

### update : Q(s<sub>4</sub>, down) = 0 + 0.1(0 + 0.8*0 - 0) = 0

<br/>

#### (Q(s<sub>7</sub>, right) = 0, maxQ(s<sub>8</sub>, right) = 0.1, R<sub>t+1</sub> = 0)

### update : Q(s<sub>7</sub>, right) = 0 + 0.1(0 + 0.8*0.1 - 0) = 0.008

<br/>

#### (Q(s<sub>8</sub>, up) = 0, maxQ(s<sub>5</sub>, not right) = 0, R<sub>t+1</sub> = 0)

### update : Q(s<sub>8</sub>, up) = 0 + 0.1(0 + 0.8*(0) - 0) = 0

<br/>

#### (Q(s<sub>5</sub>, right) = -1, maxQ(s<sub>6</sub>, down) =1, R<sub>t+1</sub> = -1)

### update : Q(s<sub>5</sub>, right) = -1 + 0.1(-1 + 0.8*(1) - (-1)) = -0.92

<br/>

#### (Q(s<sub>6</sub>, down) = 1, maxQ(s<sub>9</sub>, A<sub>t+1</sub>) =0, R<sub>t+1</sub> = 1)

### update : Q(s<sub>6</sub>, down) = 1 + 0.1(1 + 0.8*(0) - (1)) = 1

<br/>

<br/>

------

*제가 올린 글에서 잘못된 부분이 있으면 제 메일로 연락주세요!*

*Reference : 파이썬과 케라스로 배우는 강화학습*