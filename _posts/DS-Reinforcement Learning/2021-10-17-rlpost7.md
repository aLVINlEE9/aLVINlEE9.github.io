---
layout: post
title: Value Iteration(가치 이터레이션)
comments: true
categories: [DataScience/Reinforcement Learning]
tags: [value iteration, 가치 이터레이션, Reinforcement Learning, 강화학습, rl]
date: 2021-10-17 22:00:00

---

<br/>

우리가 방금까지 policy iteration을 실제로 계산 까지 해보았다.

잠시 저번 포스트의 policy iteratoin을 6번 한 결과를 보겠다.

|  s<sub>1</sub> (v = 0.512)   |  s<sub>2</sub> (v = 0.64)   | s<sub>3</sub> (v =  0.512) |
| :--------------------------: | :-------------------------: | :------------------------: |
| **s<sub>4</sub> (v = 0.64)** | **s<sub>5</sub> (v = 0.8)** | **s<sub>6</sub> (v = 1)**  |
| **s<sub>7</sub> (v = 0.8)**  |  **s<sub>8</sub> (v = 1)**  | **s<sub>9</sub> (v = 0)**  |

|  ↓   |  ↓   |  ←   |
| :--: | :--: | :--: |
|  ↓   |  ↓   |  ↓   |
|  →   |  →   |      |

이렇게 보니 value값만 보고 증가하는 추세를 따라 agent가 이동하면 될것 같아 보인다. 

정책의 필요성을 굳이 못느끼게 된다.

그래서 나오게 된것이 value iteration 인 것이다.

value itertation은 벨만 최적 방정식을 통해 푸는것인데, 따로 정책이 필요 없다.

이 벨만 최적 방정식을 계산하는 방법인 value iteartion을 이번 포스트에서 설명할 예정이다.



<br/>

# Value Iteration



저번 포스트에서 우리는 벨만 기대 방정식과 벨만 최적 방정식에 대해서 배우고 policy iteration 까지 배웠었다.

### <center>V<sub>π</sub>(s) = E[R<sub>t+1</sub> + γV<sub>π</sub>(S<sub>t+1</sub>) | S<sub>t</sub> = s]</center>

벨만 기대 방정식을 계산하면 현재 policy의 참 value 값을 구할 수 있으며,

우리는 policy iteration을 통해 다시 policy를 업데이트 하고 value를 구하고를 반복해서 최적의 정책을 구하였다,

그러나 벨만 최적방정식은 바로 최적 가치함수를 구할 수 있으며 최적 정책도 찾을 수 있다.

### <center>V<sub>*</sub>(s) = max<sub>a</sub>E[R<sub>t+1</sub> + γV<sub> *</sub>(S<sub>t+1</sub>) | S<sub>t</sub> = s, A<sub>t</sub> = a]</center>

한마디로 policy없이 구할 수 있다는 것이다. 또한 식에서 π 가 없으니 정책이 필요 없다는 것도 알 수 있다.

그럼 어떻게 구할까??

value iteration은 현재 state에서 policy를 고려하지 않고 가장 유리한 state로 갈때의 가치함수를 사용하여 현재 state의 value를 추정한다.

한마디로 가능한 action중 가장 좋은 action이 결정되었을때 value를 추정하게 된다.

<br/>

# Value Iteration in grid-world

## Iteration



현재 grid world의 상황이다.

| **v = 0** | **v = 0** |       **v = 0**        |
| :-------: | :-------: | :--------------------: |
| **v = 0** | **v = 0** | **v = 0** Trap(R = -1) |
| **v = 0** | **v = 0** | **v = 0** Goal(R = 1)  |

### <center>V<sub>k+1</sub>(s) = max<sub>a∈A</sub>(r(S<sub>t</sub> = s, A<sub>t</sub> = a) + γV<sub> k</sub>(s<sup>'</sup>))</center>

### *1st Iteration*

|   s<sub>1</sub> (v = 0)   |   s<sub>2</sub> (v = 0)   |   s<sub>3</sub> (v = 0)   |
| :-----------------------: | :-----------------------: | :-----------------------: |
| **s<sub>4</sub> (v = 0)** | **s<sub>5</sub> (v = 0)** | **s<sub>6</sub> (v = 1)** |
| **s<sub>7</sub> (v = 0)** | **s<sub>8</sub> (v = 1)** | **s<sub>9</sub> (v = 0)** |

###### V(s<sub>1</sub>) = 0 + 0.8 * 0 = 0

###### V(s<sub>2</sub>) = 0 + 0.8 * 0 = 0

###### V(s<sub>3</sub>) = 0 + 0.8 * 0 = 0

###### V(s<sub>4</sub>) = 0 + 0.8 * 0 = 0

###### V(s<sub>5</sub>) = 0 + 0.8 * 0 = 0

###### V(s<sub>6</sub>) = 1 + 0.8 * 0 = 1 (q(down))

###### V(s<sub>7</sub>) = 0 + 0.8 * 0 = 0

###### V(s<sub>8</sub>) = 1 + 0.8 * 0 = 1 (q(right))

### *2nd Iteration*

|    s<sub>1</sub> (v = 0)    |    s<sub>2</sub> (v = 0)    |   s<sub>3</sub> (v = 0)   |
| :-------------------------: | :-------------------------: | :-----------------------: |
|  **s<sub>4</sub> (v = 0)**  | **s<sub>5</sub> (v = 0.8)** | **s<sub>6</sub> (v = 1)** |
| **s<sub>7</sub> (v = 0.8)** |  **s<sub>8</sub> (v = 1)**  | **s<sub>9</sub> (v = 0)** |

###### V(s<sub>1</sub>) = 0 + 0.8 * 0 = 0

###### V(s<sub>2</sub>) = 0 + 0.8 * 0 = 0

###### V(s<sub>3</sub>) = 0 + 0.8 * 0 = 0

###### V(s<sub>4</sub>) = 0 + 0.8 * 0 = 0

###### V(s<sub>5</sub>) = 0 + 0.8 * 1 = 0.8 (q(down))

###### V(s<sub>6</sub>) = 1 + 0.8 * 0 = 1 (q(down))

###### V(s<sub>7</sub>) = 0 + 0.8 * 1 = 0.8 (q (right))

###### V(s<sub>8</sub>) = 1 + 0.8 * 0 = 1 (q(right))

### *3rd Iteration*

|    s<sub>1</sub> (v = 0)     |  s<sub>2</sub> (v = 0.64)   |   s<sub>3</sub> (v = 0)   |
| :--------------------------: | :-------------------------: | :-----------------------: |
| **s<sub>4</sub> (v = 0.64)** | **s<sub>5</sub> (v = 0.8)** | **s<sub>6</sub> (v = 1)** |
| **s<sub>7</sub> (v = 0.8)**  |  **s<sub>8</sub> (v = 1)**  | **s<sub>9</sub> (v = 0)** |

###### V(s<sub>1</sub>) = 0 + 0.8 * 0 = 0

###### V(s<sub>2</sub>) = 0 + 0.8 * 0.8 = 0.64 (q(down))

###### V(s<sub>3</sub>) = 0 + 0.8 * 0 = 0

###### V(s<sub>4</sub>) = 0 + 0.8 * 0.8 = 0.64 (q(right or down))

###### V(s<sub>5</sub>) = 0 + 0.8 * 1 = 0.8 (q(down))

###### V(s<sub>6</sub>) = 1 + 0.8 * 0 = 1 (q(down))

###### V(s<sub>7</sub>) = 0 + 0.8 * 1 = 0.8 (q (right))

###### V(s<sub>8</sub>) = 1 + 0.8 * 0 = 1 (q(right))

### *4th Iteration*

|  s<sub>1</sub> (v = 0.512)   |  s<sub>2</sub> (v = 0.64)   | s<sub>3</sub> (v = 0.512) |
| :--------------------------: | :-------------------------: | :-----------------------: |
| **s<sub>4</sub> (v = 0.64)** | **s<sub>5</sub> (v = 0.8)** | **s<sub>6</sub> (v = 1)** |
| **s<sub>7</sub> (v = 0.8)**  |  **s<sub>8</sub> (v = 1)**  | **s<sub>9</sub> (v = 0)** |

###### V(s<sub>1</sub>) = 0 + 0.8 * 0.64 = 0.512 (q(right or down))

###### V(s<sub>2</sub>) = 0 + 0.8 * 0.8 = 0.64 (q(down))

###### V(s<sub>3</sub>) = 0 + 0.8 * 0.64 = 0.512 (q(left))

###### V(s<sub>4</sub>) = 0 + 0.8 * 0.8 = 0.64 (q(right or down))

###### V(s<sub>5</sub>) = 0 + 0.8 * 1 = 0.8 (q(down))

###### V(s<sub>6</sub>) = 1 + 0.8 * 0 = 1 (q(down))

###### V(s<sub>7</sub>) = 0 + 0.8 * 1 = 0.8 (q (right))

###### V(s<sub>8</sub>) = 1 + 0.8 * 0 = 1 (q(right))

<br/>

## Print Policy

|  s<sub>1</sub> (v = 0.512)   |  s<sub>2</sub> (v = 0.64)   | s<sub>3</sub> (v = 0.512) |
| :--------------------------: | :-------------------------: | :-----------------------: |
| **s<sub>4</sub> (v = 0.64)** | **s<sub>5</sub> (v = 0.8)** | **s<sub>6</sub> (v = 1)** |
| **s<sub>7</sub> (v = 0.8)**  |  **s<sub>8</sub> (v = 1)**  | **s<sub>9</sub> (v = 0)** |

### **s<sub>1</sub>** 

**q(s<sub>1</sub>, right) = 0 + 0.8(0.64) = 0.512**

**q(s<sub>1</sub>, down) = 0 + 0.8(0.64) = 0.512**

### **s<sub>2</sub>** 

q(s<sub>2</sub>, left) = 0 + 0.8(0.512) = 0.4096

**q(s<sub>2</sub>, down) = 0 + 0.8(0.8) = 0.64**

q(s<sub>2</sub>, down) = 0 + 0.8(0.512) = 0.4096

### **s<sub>3</sub>** 

**q(s<sub>3</sub>, left) = 0 + 0.8(0.64) = 0.512**

q(s<sub>3</sub>, down) = -1 + 0.8(1) = -0.2

### **s<sub>4</sub>** 

q(s<sub>4</sub>, up) = 0 + 0.8(0.512) = 0.4096

**q(s<sub>4</sub>, right) = 0 + 0.8(0.8) = 0.64**

**q(s<sub>4</sub>, down) = 0 + 0.8(0.8) = 0.64**

### **s<sub>5</sub>** 

q(s<sub>5</sub>, up) = 0 + 0.8(0.64) = 0.512

**q(s<sub>5</sub>, down) = 0 + 0.8(1) = 0.8**

q(s<sub>5</sub>, left) = 0 + 0.8(0.64) = 0.512

q(s<sub>5</sub>, right) = -1 + 0.8(1) = -0.2

### **s<sub>6</sub>** 

q(s<sub>6</sub>, up) = 0 + 0.8(0.512) = 0.4096

**q(s<sub>6</sub>, down) = 1 + 0.8(0) = 1**

q(s<sub>6</sub>, up) = 0 + 0.8(0.8) = 0.64

### **s<sub>7</sub>** 

q(s<sub>7</sub>, up) = 0 + 0.8(0.64) = 0.512

**q(s<sub>7</sub>, right) = 0 + 0.8(1) = 0.8**

### **s<sub>8</sub>** 

q(s<sub>8</sub>, up) = 0 + 0.8(0.8) = 0.64

q(s<sub>8</sub>, left) = 0 + 0.8(0.8) = 0.64

**q(s<sub>8</sub>, right) = 1 + 0.8(0) = 1**

<br/>

## Value Iteration으로 얻은 정책

| ↓ →  |  ↓   |  ←   |
| :--: | :--: | :--: |
| ↓ →  |  ↓   |  ↓   |
|  →   |  →   |      |

<br/>

------

*제가 올린 글에서 잘못된 부분이 있으면 제 메일로 연락주세요!*

*Reference : 파이썬과 케라스로 배우는 강화학습*